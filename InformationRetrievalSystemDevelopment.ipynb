{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37544ff8",
   "metadata": {},
   "source": [
    "# Information Retrieval System Developmen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c19a40e",
   "metadata": {},
   "source": [
    "# 1. Dataset Selection and Preparation:\n",
    " - Your first task is to select a dataset of text documents. This dataset should represent a collection of documents from a specific domain (e.g., news articles, research papers, or product descriptions).\n",
    " - Ensure that the dataset is appropriately labeled or categorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2ebf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f31ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Post</th>\n",
       "      <th>ID_Annotator</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3326</td>\n",
       "      <td>1</td>\n",
       "      <td>ArgumentsUsed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3326</td>\n",
       "      <td>1</td>\n",
       "      <td>Discriminating</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3326</td>\n",
       "      <td>1</td>\n",
       "      <td>Inappropriate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3326</td>\n",
       "      <td>1</td>\n",
       "      <td>OffTopic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3326</td>\n",
       "      <td>1</td>\n",
       "      <td>PersonalStories</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58563</th>\n",
       "      <td>1003437</td>\n",
       "      <td>1</td>\n",
       "      <td>PossiblyFeedback</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58564</th>\n",
       "      <td>1004625</td>\n",
       "      <td>1</td>\n",
       "      <td>PossiblyFeedback</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58565</th>\n",
       "      <td>1006255</td>\n",
       "      <td>1</td>\n",
       "      <td>PossiblyFeedback</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58566</th>\n",
       "      <td>1010868</td>\n",
       "      <td>2</td>\n",
       "      <td>PossiblyFeedback</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58567</th>\n",
       "      <td>1010997</td>\n",
       "      <td>1</td>\n",
       "      <td>PossiblyFeedback</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58568 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID_Post  ID_Annotator          Category  Value\n",
       "0         3326             1     ArgumentsUsed      0\n",
       "1         3326             1    Discriminating      0\n",
       "2         3326             1     Inappropriate      0\n",
       "3         3326             1          OffTopic      0\n",
       "4         3326             1   PersonalStories      0\n",
       "...        ...           ...               ...    ...\n",
       "58563  1003437             1  PossiblyFeedback      0\n",
       "58564  1004625             1  PossiblyFeedback      1\n",
       "58565  1006255             1  PossiblyFeedback      0\n",
       "58566  1010868             2  PossiblyFeedback      0\n",
       "58567  1010997             1  PossiblyFeedback      1\n",
       "\n",
       "[58568 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('Annotations.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a9488",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing:\n",
    " - The next step involves cleaning and preprocessing the text data. This includes tasks such as:\n",
    " - Tokenization.\n",
    " - Lowercasing.\n",
    " - Removing punctuation.\n",
    " - Handling missing values, if any.\n",
    " - You should also create an inverted index, a data structure that maps terms (words) to their \n",
    "corresponding documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceaf8120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['category']\n",
      "Filtered Tokens (Stopword Removal): ['category']\n",
      "Stemmed Tokens: ['categori']\n",
      "Lemmatized Tokens: ['category']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize('Category')\n",
    "\n",
    "# Lowercasing\n",
    "tokens = [word.lower() for word in tokens]\n",
    "\n",
    "# Removing Punctuation\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "tokens = [word.translate(table) for word in tokens]\n",
    "\n",
    "# Handling Missing Values (not applicable in this example)\n",
    "\n",
    "# Stop Word Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "# Normalization (not shown here, as it depends on specific use cases)\n",
    "\n",
    "# Encoding and Vectorization (not shown here, as it depends on specific use cases)\n",
    "\n",
    "# Printing the results\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Filtered Tokens (Stopword Removal):\", filtered_tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e38f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents to TF-IDF vectors\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc24e38",
   "metadata": {},
   "source": [
    "# 3. User Query Interface:\n",
    " - Create a user-friendly query interface that allows users to input search queries.\n",
    " - Ensure that the interface can handle natural language queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77c0892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\hania fatima\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hania fatima\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hania fatima\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hania fatima\\anaconda3\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hania fatima\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\hania fatima\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\hania fatima\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\hania fatima\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\hania fatima\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\hania fatima\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\hania fatima\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c18f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Query Processing\n",
    "user_query = \"This is the second document.\"  # Replace with the actual user query\n",
    "\n",
    "# Preprocess the user query using the same preprocessing steps as used for documents\n",
    "# If you don't have a separate function, you can apply the preprocessing directly here\n",
    "# For example, assuming you tokenize and remove stopwords:\n",
    "user_query_tokens = user_query.split()  # Tokenization\n",
    "user_query_tokens = [word.lower() for word in user_query_tokens]  # Lowercasing\n",
    "user_query_tokens = [word for word in user_query_tokens if word not in stop_words]  # Stopword removal\n",
    "\n",
    "# Join the preprocessed tokens back into a single string\n",
    "preprocessed_query = \" \".join(user_query_tokens)\n",
    "\n",
    "# Compute the TF-IDF vector for the query\n",
    "query_vector = tfidf_vectorizer.transform([preprocessed_query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e6b11",
   "metadata": {},
   "source": [
    "# 4. Retrieval Algorithm:\n",
    " - Implement the Vector Space Model (VSM) or Term Frequency-Inverse Document Frequency (TF-IDF) as your retrieval algorithm. These are beginner-friendly approaches.\n",
    " - Your system should rank documents based on their relevance to user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb49c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def preprocess_query(query):\n",
    "    # Tokenization and lowercasing\n",
    "    tokens = query.lower().split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_query = \" \".join(tokens)\n",
    "    \n",
    "    return preprocessed_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9fbafc",
   "metadata": {},
   "source": [
    "# 5. Query Processing:\n",
    " - Preprocess user queries similarly to how you processed documents.\n",
    " - The system should compare the user query to the indexed documents and return a ranked list of relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "856a0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess user query\n",
    "user_query = \"This is the second document.\"  # Replace with the user's query\n",
    "preprocessed_user_query = preprocess_query(user_query)\n",
    "\n",
    "# Compute the TF-IDF vector for the query\n",
    "query_vector = tfidf_vectorizer.transform([preprocessed_user_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a6584eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Print the ranked documents\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, score \u001b[38;5;129;01min\u001b[39;00m sorted_documents:\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocuments[idx]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity between the query vector and document vectors\n",
    "cosine_similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "\n",
    "# Get a list of document indexes sorted by relevance\n",
    "document_scores = list(enumerate(cosine_similarities[0]))\n",
    "sorted_documents = sorted(document_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the ranked documents\n",
    "for idx, score in sorted_documents:\n",
    "    print(f\"Document {idx + 1}: {documents[idx]}, Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75badf13",
   "metadata": {},
   "source": [
    "# 6. Evaluation:\n",
    " - Define a set of test queries and relevant documents to evaluate the system's performance.\n",
    " - Use common IR evaluation metrics like Precision, Recall, and F1-score to assess how well the system retrieves relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "595d4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"How does climate change impact ecosystems?\",\n",
    "    \"Latest advances in artificial intelligence\",\n",
    "    \"Healthy diet for weight loss\",\n",
    "    \"How does blockchain technology work?\",\n",
    "    \"Treatment options for diabetes\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74db5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ground truth for each test query as a list of document indexes\n",
    "ground_truth = {\n",
    "    \"How does climate change impact ecosystems?\": [1, 3, 6],\n",
    "    \"Latest advances in artificial intelligence\": [2, 4, 5],\n",
    "    \"Healthy diet for weight loss\": [8, 10, 12],\n",
    "    \"How does blockchain technology work?\": [14, 16, 18],\n",
    "    \"Treatment options for diabetes\": [20, 22, 24],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ac12193",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieve_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Iterate through test queries\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m test_queries:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Retrieve documents for the query using your IR system (replace with your code)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     retrieved_documents \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_documents\u001b[49m(query)  \u001b[38;5;66;03m# Implement this function\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Get the ground truth for this query\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     relevant_documents \u001b[38;5;241m=\u001b[39m ground_truth\u001b[38;5;241m.\u001b[39mget(query, [])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retrieve_documents' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize lists to store evaluation results\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate through test queries\n",
    "for query in test_queries:\n",
    "    # Retrieve documents for the query using your IR system (replace with your code)\n",
    "    retrieved_documents = retrieve_documents(query)  # Implement this function\n",
    "\n",
    "    # Get the ground truth for this query\n",
    "    relevant_documents = ground_truth.get(query, [])\n",
    "\n",
    "    # Calculate Precision, Recall, and F1-score\n",
    "    precision = precision_score(relevant_documents, retrieved_documents)\n",
    "    recall = recall_score(relevant_documents, retrieved_documents)\n",
    "    f1 = f1_score(relevant_documents, retrieved_documents)\n",
    "\n",
    "    # Append the scores to the lists\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate average scores\n",
    "average_precision = sum(precision_scores) / len(precision_scores)\n",
    "average_recall = sum(recall_scores) / len(recall_scores)\n",
    "average_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "# Print the average scores\n",
    "print(\"Average Precision:\", average_precision)\n",
    "print(\"Average Recall:\", average_recall)\n",
    "print(\"Average F1-score:\", average_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b24fa89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
